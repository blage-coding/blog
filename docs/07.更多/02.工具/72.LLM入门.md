---
title: LLM入门
date: 2024-01-04 16:49:48
permalink: /pages/a29837/
categories:
  - 更多
  - 工具
tags:
  - 工具
author: 
  name: phan
  link: https://github.com/blage-coding
---
# LLM入门

## 提示工程

- 清晰、具体的指令提示
  - 区分指令和文本内容：使用特殊的符号区分指令、输入文本内容
  - 指定输出的格式：让模型将回答转化为结构化格式输出，比如直接返回json格式
  - 让模型能够针对问题自主进行校验和判断，类似于if-else执行输出不同的结果
  - few-shot少样本提示：提供几个样例，告诉模型正确的输出格式
  - 拆分任务，明确步骤：指定完成任务所需要的步骤
  - 引导模型自主思考，做出判断并给出思路
- 迭代优化
  - 输出长度限制
  - 指明关注内容
  - 将输出内容格式化为表格和HTML格式
- 文本概括
  - 长度限制：“生成某个评论的简短摘要，最多不超过30词”
  - 输出侧重的角度不同：让模型输出的内容可以侧重关注某个方面
  - 命令模型只提取输出某个关键信息
  - 多段长文本叙述：for循环依次概括每一段文本，拼接最终结果
- 情感推断&主题判断
  - **模型识别分类，并指定类别名称进行输出，方便后续的处理**
  - 指定json格式、类别名称
- 文本转换
  - 需要在高质量平行语料上Fine-Tune
  - 格式转换：HTML转换JSON
  - 语法错误、格式纠错
- 文本扩展
  - 根据输入的短文本，生成更加丰富的长文本
  - 通过设置temperature来控制生成质量，值越小则输出概率最高的内容，每轮输出的内容越稳定；值越大则输出更多样化，输出的内容不稳定
  - **多次利用大模型，输出得到更多原始文本相关的信息，比如情感、主题，最后和原始文本一并输入模型进行提问**
- 不同角色——聊天问答系统
  - role——system：**设置assistant的行为、角色扮演、整个问答期望的输入和输出内容和格式**。指定整个完整的对话流程，包括接收到用户的消息之后，输出什么内容进行回答
  - role——assistant和user来进行交互和对话

## 基于ChatGPT的问答系统

- 输入内容校验，防止prompt注入
  - 使用分隔符、换行符、用户输入拼接上校验信息作为user_message
  - system当中定义校验的非法类型，通过判断用户输入来判断是否属于非法类型，并进行处理和返回
- 思维链
  - system声明每个步骤进行怎样的推理，并可以让模型输出每个步骤的推理
  - 隐藏中间推理过程，只返回用户关心的输出内容
  - step by step提示链：拆分成子任务多轮交互，成功率更高。其中每个拆分成的子任务执行方式可以有多种：
    1. LLM交互
    2. 数据库
    3. 程序API控制
  - 知识检索：**没有必要将所有信息库内容都加入system当中，可以按需动态加载与用户问题相关的知识**
    - 将所有知识内容存入json文件当中，然后根据问题按需读取
    - 文本嵌入embedding，高效知识检索，实现**模糊或语义搜索**
- 输出评估
  - 使用API输出敏感类型的得分，并根据是否有害输出新的响应或是替代答案(没必要)
  - **启动另一个system角色，和模型交互来判断前一个回答是否符合要求**
- 端到端问答系统
  1. 输入信息进行校验
  2. 相关知识检索：根据输入判断属于哪个类别、主题、方向(模型交互、正则表达式)，然后根据这个方向进行详细的知识检索
  3. 模型问答&少样本提示：user——原始问题的输入；assistant——提供辅助(**详细知识**)
  4. 输出质量校验和反馈
- prompt效果评估
  - 指定输出格式、少样本提示few_shot
  - 回归测试：确保改进后的prompt engineer不会对先前的样例造成影响
  - 效果评估
    - 对于有准确答案的问题，计算每次回答的准确率
    - **对于答案比较模糊的问题，交给LLM进行评估**：
      - 提供一个专家理想的答案
      - 交给LLM评估代理回答和专家回答之间的相似性，关注在内容上，忽略样式、语法、细微差异

## LangChain程序开发

- 提示模板
  - from_template：构造提示模板
  - format_messages：将变量插入提示模板字符串
  - 调用chat方法进行模型交互
  - 设置输出解释器format_instructions，将返回的str类型输出通过解析器格式化为JSON
- 储存memory：保存整个对话的上下文对话历史
  - 外部存储上下文信息ConversationBufferMemory
  - 对话缓存窗口存储ConversationBufferWindowMemory：只保存上一轮的对话记录
  - 对话字符缓存储存ConversationTokenBufferMemory：内存限制保存的tokne数，超出则删除最早对话的token
  - 对话摘要缓存存储ConversationSummaryBufferMemory
- LLM链：将模型和prompt结合构成LLMChain对象
  - 简单顺序链：多个LLM链拼接在一起，并顺序执行。每个链模板输入和输出只有一个，**前一个链的输出作为下一个链条模板的输入**。
  - 顺序链：通过指定输出结果的模板key名称，实现多输入和多输出的效果，任意**组合排列**不同模型链的结果。
  - 路由链：根据当前链的输出结果，选择下一个要调用哪一个链，
    - 定义多个目标链+默认链(不能明确该路由哪个目标链时，则调用默认链)
    - **定义一个路由链，并和多提示的模板进行绑定**。<font color="red">**本质上也是将输入内容通过LLM进行交互，判断出和哪个目标链相似**</font>(属于哪个目标链的类别)
    - 路由输出解析对象RouterOutputParser
- 基于文档问答
  - 直接使用向量数据库进行查询
  - Embeddings+向量存储索引：指定向量表征模型OpenAIEmbeddings+文档列表构建向量数据库。并调用similarity_search进行相似搜索
  - 检索问答链：
    - 指定LLM负责文本生成
    - 指定传入链类型(根据什么方式使用文档中的相关块信息进行问答)：
      - stuff：查询得到的文档组合成一个文档传入下一步
      - map_reduce：对于文档每个块的知识信息，都调用LLM交互询问（并行）。最后再使用LLM将所有回答总结成答案
      - Refine：循坏提问的方式，前n个块的回答结果会和第n+1个块，一并输入到LLM当中进行问答
      - Map_rerank：每个块调用LLM进行询问，并要求模型输出结果包含一个可信度分数，最终选择分数最高的回答作为结果。
  - **自动化评估**：基于QAGenerateChain自动化生成样本的测试集。本质上还是根据输入的向量知识块**，利用定制化prompt基于LLM生成问答测试集**，其中提示词当中会有\<基于给出的文本知识，生成问答对>相关的字词。
- Agent代理
  - 创建LLM、Tool(包括计算器工具、维基百科，PythonREPLTool，自定义的工具函数——需要注释)、Agent
  - **本质上也是通过多轮LLM，输出判断使用哪个工具？然后Agent会调用对应的Tool函数，再将结果输入模型询问**。从而利用外部工具获取更加有价值的知识，与模型进行交互

## LangChain访问个人数据

- 文档加载：PDF、音频、markdown等等文档
- 文档分割：按照长度分割、按照某个字符进行分割、递归字符分割(对每段文本依次使用每个分割符进行分割)、Token（不支持中文）、Markdown——根据标题分割
- 向量数据库+词向量
  - 获取每个chunk进行Embeddings，相近内容的向量在嵌入空间比较接近。
  - 持久化
- retrieval检索
  - similarity_search相似性搜索
  - MMR最大边际相关性：去除冗余的信息、提供多样性数据(相关度，相似度——**适合多样性需求文本**)
  - 元数据filter：过滤掉无用的文本。但是需要手动设置。另一种方法是**通过LLM交互**，通过LLM提取对应的敏感过滤信息
  - 压缩：通过LLM获取得到压缩后的chunk，压缩得到的知识只和问题相关
- 上下文问题
  - 讲历史对话信息和当前查询合并，再将整个内容从向量数据库进行检索